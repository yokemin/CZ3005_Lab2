{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Woon_Yoke_Min_U1921797E_Lab2_Codes.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhAdHX6As7gB"
      },
      "source": [
        "# Task 1\n",
        "You are asked to build a three-layer feed-forward neural network to solve the monitoring problem of injection molding machine. Your implementation must be in Pytorch and executable in Google Colab environments. The proportion of training and testing samples is 70:30 where your model must deliver the smallest testing error possible. In that case, you need to select the number of nodes of hidden\n",
        "layers, the number of epochs, the learning rates, the mini-batch size, etc. that lead to the smallest testing error. In this assignment, you have to use the SGD optimizer as exemplified in the lab materials under the mini-batch update fashion. The evaluation metric here is the classification error. No feature selection is allowed here."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fGTB1TSubhi"
      },
      "source": [
        "## Import OQC.mat file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "resources": {
            "http://localhost:8080/nbextensions/google.colab/files.js": {
              "data": "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",
              "ok": true,
              "headers": [
                [
                  "content-type",
                  "application/javascript"
                ]
              ],
              "status": 200,
              "status_text": ""
            }
          },
          "base_uri": "https://localhost:8080/",
          "height": 73
        },
        "id": "thtRGZbsOnI3",
        "outputId": "5286d9be-b9d7-4ad4-aa87-0a545f213bd4"
      },
      "source": [
        "from google.colab import files\n",
        "uploaded = files.upload()\n",
        "\n",
        "from scipy.io import loadmat\n",
        "oqc = loadmat('OQC.mat')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d55cdf6b-84d0-4587-86a4-45b62698482f\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d55cdf6b-84d0-4587-86a4-45b62698482f\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script src=\"/nbextensions/google.colab/files.js\"></script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving OQC.mat to OQC.mat\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MPJ2RaoIPhoH",
        "outputId": "2c9d9e60-c7c6-450c-e10c-1fa737d17dad"
      },
      "source": [
        "oqc.keys()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['__header__', '__version__', '__globals__', 'data'])"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZdYMdwmTPkC5",
        "outputId": "793c5f09-8bbf-4a51-dcc0-7a242d91c856"
      },
      "source": [
        "type(oqc['data']), oqc['data'].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.ndarray, (2952, 49))"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n73Od24nQHYx",
        "outputId": "494c91e8-0b9f-4562-a802-cdbe1422d4d3"
      },
      "source": [
        "type(oqc['data'][0][0]), oqc['data'][0][0].shape"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(numpy.float64, ())"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ywGU6M3kSevM"
      },
      "source": [
        "## Load Data into Python DataFrame\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m3VXVQLCQHbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fc4582c5-4389-486a-92d8-56595ede210e"
      },
      "source": [
        "import pandas as pd\n",
        "columns = ['1','2', '3','4','5','6','7','8','9','10','11','12','13','14','15','16','17','18','19','20','21','22','23','24','25','26','27','28','29','30','31','32','33','34','35','36','37','38','39','40','41','42','43','44','45','46','47','48','Defect']\n",
        "df = pd.DataFrame(oqc['data'], columns=columns)\n",
        "df"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>40</th>\n",
              "      <th>41</th>\n",
              "      <th>42</th>\n",
              "      <th>43</th>\n",
              "      <th>44</th>\n",
              "      <th>45</th>\n",
              "      <th>46</th>\n",
              "      <th>47</th>\n",
              "      <th>48</th>\n",
              "      <th>Defect</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.390525</td>\n",
              "      <td>0.147798</td>\n",
              "      <td>0.156398</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.350680</td>\n",
              "      <td>0.995993</td>\n",
              "      <td>0.787486</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.375</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.078025</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.066666</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.614525</td>\n",
              "      <td>0.385475</td>\n",
              "      <td>0.921937</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.033465</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.233803</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.980563</td>\n",
              "      <td>0.467565</td>\n",
              "      <td>0.616027</td>\n",
              "      <td>0.070283</td>\n",
              "      <td>0.719864</td>\n",
              "      <td>0.037255</td>\n",
              "      <td>0.329627</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.078970</td>\n",
              "      <td>0.079625</td>\n",
              "      <td>0.922469</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.390525</td>\n",
              "      <td>0.147798</td>\n",
              "      <td>0.156398</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.350680</td>\n",
              "      <td>0.995993</td>\n",
              "      <td>0.787486</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.078025</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.066666</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.614525</td>\n",
              "      <td>0.385475</td>\n",
              "      <td>0.921937</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.068898</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.233803</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.918348</td>\n",
              "      <td>0.451597</td>\n",
              "      <td>0.616027</td>\n",
              "      <td>0.096282</td>\n",
              "      <td>0.719864</td>\n",
              "      <td>0.072549</td>\n",
              "      <td>0.328711</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.053677</td>\n",
              "      <td>0.053677</td>\n",
              "      <td>0.921482</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.508494</td>\n",
              "      <td>0.147798</td>\n",
              "      <td>0.156398</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.331065</td>\n",
              "      <td>0.995993</td>\n",
              "      <td>0.787486</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.079506</td>\n",
              "      <td>0.305174</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.066666</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.614525</td>\n",
              "      <td>0.385475</td>\n",
              "      <td>0.920455</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.233803</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.623919</td>\n",
              "      <td>0.493513</td>\n",
              "      <td>0.635777</td>\n",
              "      <td>0.054857</td>\n",
              "      <td>0.842105</td>\n",
              "      <td>0.994118</td>\n",
              "      <td>0.326533</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.920494</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.577954</td>\n",
              "      <td>0.147798</td>\n",
              "      <td>0.156398</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.327862</td>\n",
              "      <td>0.995993</td>\n",
              "      <td>0.787486</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.079012</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.066666</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.614525</td>\n",
              "      <td>0.385475</td>\n",
              "      <td>0.920949</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.233803</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.670883</td>\n",
              "      <td>0.480539</td>\n",
              "      <td>0.637676</td>\n",
              "      <td>0.051998</td>\n",
              "      <td>0.877759</td>\n",
              "      <td>0.994118</td>\n",
              "      <td>0.326533</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.056206</td>\n",
              "      <td>0.057424</td>\n",
              "      <td>0.920494</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.649490</td>\n",
              "      <td>0.147798</td>\n",
              "      <td>0.156398</td>\n",
              "      <td>0.002560</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.500000</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.331065</td>\n",
              "      <td>0.995993</td>\n",
              "      <td>0.787486</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.375</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.080000</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.066666</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.614525</td>\n",
              "      <td>0.385475</td>\n",
              "      <td>0.919961</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.237900</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.649028</td>\n",
              "      <td>0.463074</td>\n",
              "      <td>0.634637</td>\n",
              "      <td>0.051911</td>\n",
              "      <td>0.874363</td>\n",
              "      <td>0.996078</td>\n",
              "      <td>0.323210</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.057705</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.920000</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2947</th>\n",
              "      <td>0.188184</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.772512</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.777778</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>0.50</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.350680</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.977913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.500</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.066667</td>\n",
              "      <td>0.305174</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.694826</td>\n",
              "      <td>0.616387</td>\n",
              "      <td>0.383613</td>\n",
              "      <td>0.933301</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.049213</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.237388</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.983818</td>\n",
              "      <td>0.759980</td>\n",
              "      <td>0.616027</td>\n",
              "      <td>0.187971</td>\n",
              "      <td>0.767402</td>\n",
              "      <td>0.052941</td>\n",
              "      <td>0.272437</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.203185</td>\n",
              "      <td>0.269508</td>\n",
              "      <td>0.933333</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2948</th>\n",
              "      <td>0.471121</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.772512</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.335468</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.977913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.068148</td>\n",
              "      <td>0.307286</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.692714</td>\n",
              "      <td>0.616387</td>\n",
              "      <td>0.383613</td>\n",
              "      <td>0.931818</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.230986</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.624291</td>\n",
              "      <td>0.472056</td>\n",
              "      <td>0.630839</td>\n",
              "      <td>0.055204</td>\n",
              "      <td>0.838710</td>\n",
              "      <td>0.998039</td>\n",
              "      <td>0.272437</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.059672</td>\n",
              "      <td>0.059672</td>\n",
              "      <td>0.931852</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2949</th>\n",
              "      <td>0.540015</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.772512</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.714286</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.785714</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.335068</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.977913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.4375</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.5</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.068642</td>\n",
              "      <td>0.305174</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.616387</td>\n",
              "      <td>0.383613</td>\n",
              "      <td>0.931324</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.236364</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.670511</td>\n",
              "      <td>0.463074</td>\n",
              "      <td>0.630080</td>\n",
              "      <td>0.051911</td>\n",
              "      <td>0.853990</td>\n",
              "      <td>0.998039</td>\n",
              "      <td>0.271634</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.056112</td>\n",
              "      <td>0.931358</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2950</th>\n",
              "      <td>0.606644</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.772512</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.329463</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.977913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.069136</td>\n",
              "      <td>0.307286</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.692714</td>\n",
              "      <td>0.612663</td>\n",
              "      <td>0.387337</td>\n",
              "      <td>0.930830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.233035</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.649772</td>\n",
              "      <td>0.466068</td>\n",
              "      <td>0.636156</td>\n",
              "      <td>0.052171</td>\n",
              "      <td>0.848896</td>\n",
              "      <td>0.998039</td>\n",
              "      <td>0.270832</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.056581</td>\n",
              "      <td>0.056581</td>\n",
              "      <td>0.930864</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2951</th>\n",
              "      <td>0.718951</td>\n",
              "      <td>0.009829</td>\n",
              "      <td>0.772512</td>\n",
              "      <td>0.000113</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.444444</td>\n",
              "      <td>0.571429</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.857143</td>\n",
              "      <td>1.0</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.329864</td>\n",
              "      <td>0.994391</td>\n",
              "      <td>0.977913</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.5000</td>\n",
              "      <td>0.625</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.666667</td>\n",
              "      <td>0.25</td>\n",
              "      <td>0.069136</td>\n",
              "      <td>0.306230</td>\n",
              "      <td>0.333333</td>\n",
              "      <td>0.5</td>\n",
              "      <td>0.166667</td>\n",
              "      <td>0.693770</td>\n",
              "      <td>0.612663</td>\n",
              "      <td>0.387337</td>\n",
              "      <td>0.930830</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.996063</td>\n",
              "      <td>0.049981</td>\n",
              "      <td>0.233291</td>\n",
              "      <td>1.0</td>\n",
              "      <td>0.662327</td>\n",
              "      <td>0.472056</td>\n",
              "      <td>0.636156</td>\n",
              "      <td>0.053211</td>\n",
              "      <td>0.853990</td>\n",
              "      <td>0.992157</td>\n",
              "      <td>0.268080</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>0.125</td>\n",
              "      <td>0.057518</td>\n",
              "      <td>0.056581</td>\n",
              "      <td>0.930864</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2952 rows Ã— 49 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "             1         2         3  ...        47        48  Defect\n",
              "0     0.390525  0.147798  0.156398  ...  0.079625  0.922469     2.0\n",
              "1     0.390525  0.147798  0.156398  ...  0.053677  0.921482     2.0\n",
              "2     0.508494  0.147798  0.156398  ...  0.056112  0.920494     2.0\n",
              "3     0.577954  0.147798  0.156398  ...  0.057424  0.920494     2.0\n",
              "4     0.649490  0.147798  0.156398  ...  0.056112  0.920000     2.0\n",
              "...        ...       ...       ...  ...       ...       ...     ...\n",
              "2947  0.188184  0.009829  0.772512  ...  0.269508  0.933333     0.0\n",
              "2948  0.471121  0.009829  0.772512  ...  0.059672  0.931852     0.0\n",
              "2949  0.540015  0.009829  0.772512  ...  0.056112  0.931358     0.0\n",
              "2950  0.606644  0.009829  0.772512  ...  0.056581  0.930864     0.0\n",
              "2951  0.718951  0.009829  0.772512  ...  0.056581  0.930864     0.0\n",
              "\n",
              "[2952 rows x 49 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QD3yLiNaW2ss",
        "outputId": "5a5ebce2-2054-4916-a878-f605063a27bf"
      },
      "source": [
        "# Check distribution of Defect types\n",
        "df[\"Defect\"].value_counts()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0    1074\n",
              "0.0    1008\n",
              "2.0     870\n",
              "Name: Defect, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Qs546W8UmPX"
      },
      "source": [
        "## Train Test Split"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sTpbgMuHSc5h"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "import time"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "A-KbySfuUvSg"
      },
      "source": [
        "X = df.iloc[:, 0:48]\n",
        "Y = df.loc[:, \"Defect\"]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlD1q6x0SdGd"
      },
      "source": [
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q1NsL2cOSdNN"
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from random import randint\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from torch.utils.data import Dataset\n",
        "from torch.utils.data import DataLoader"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ue5zeo8SSdT0"
      },
      "source": [
        "# Convert dataframe into Tensors\n",
        "X_train, X_test = map(torch.tensor, (X_train.values, X_test.values))\n",
        "Y_train, Y_test = map(torch.tensor, (Y_train.values, Y_test.values))\n",
        "\n",
        "# Make the tensors dtype as float for X values\n",
        "X_train = X_train.float()\n",
        "X_test = X_test.float()\n",
        "\n",
        "Y_train = Y_train.long()\n",
        "Y_test = Y_test.long()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qNYdQy5aY4GI",
        "outputId": "1089f4a8-54fb-4fcd-e4c2-09725ad4ae0b"
      },
      "source": [
        "print(X_train.size())\n",
        "print(X_test.size())\n",
        "print(Y_train.unique(return_counts=True))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "torch.Size([2066, 48])\n",
            "torch.Size([886, 48])\n",
            "(tensor([0, 1, 2]), tensor([709, 747, 610]))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kJvZTseIvnnZ"
      },
      "source": [
        "## Set hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4pByIPGnSdQQ"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 200  # no of nodes in hidden layers\n",
        "num_epochs = 150\n",
        "batch_size = 20\n",
        "learning_rate = 0.01"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uOuB0PQVY0Yh"
      },
      "source": [
        "## Feed Forward Neural Network"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hQbC3UY-UZUs"
      },
      "source": [
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ngoaYPvYam1T",
        "outputId": "b575c75a-f9d1-4eea-d92a-95dd5a581182"
      },
      "source": [
        "# Create network object\n",
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "print(model)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FeedForward(\n",
            "  (fc1): Linear(in_features=48, out_features=200, bias=False)\n",
            "  (fc2): Linear(in_features=200, out_features=200, bias=False)\n",
            "  (fc3): Linear(in_features=200, out_features=3, bias=False)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e0oOBJhxlYS6"
      },
      "source": [
        "##Train the Model and Get Classification error on Test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AFQvwB_Mk-E9"
      },
      "source": [
        "def get_error(outputs, labels):\n",
        "\n",
        "    batch_size=outputs.size(0)\n",
        "    predicted_labels = outputs.argmax(dim = 1)\n",
        "    indicator = (predicted_labels == labels)\n",
        "    num_matches = indicator.sum()\n",
        "    \n",
        "    return 1 - num_matches.float()/batch_size "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ahukXWEYk-E9"
      },
      "source": [
        "def eval_on_test_set():\n",
        "\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "\n",
        "    for i in range(0, X_test.size()[0], batch_size):\n",
        "\n",
        "        minibatch_data = X_test[i:i+batch_size]\n",
        "        minibatch_label= Y_test[i:i+batch_size]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),784)\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        error = get_error(outputs, minibatch_label)\n",
        "        running_error += error.item()\n",
        "\n",
        "        num_batches += 1\n",
        "\n",
        "    total_error = running_error/num_batches\n",
        "    print(\"Test error: {:.5f} %\".format(total_error * 100))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrNc-reml4nf"
      },
      "source": [
        "# Create a loss criteria\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "# Create an optimizer (SGD optimizer) \n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJtLilcxVi2-",
        "outputId": "8a9e366d-dc74-4e67-cd3e-adc9c59b93d0"
      },
      "source": [
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # learning rate strategy : divide the learning rate by 1.5 every 10 epochs\n",
        "    if epoch % 10 == 0 and epoch > 10: \n",
        "        learning_rate = learning_rate / 1.5\n",
        "    \n",
        "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
        "    optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "        \n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "        \n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\"\n",
        "    # by the number of batches\n",
        "    \n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats \n",
        "    # and compute the error rate on the test set  \n",
        "    \n",
        "    if epoch % 10 == 0 : \n",
        "    \n",
        "        print(' ')\n",
        "        \n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        \n",
        "        eval_on_test_set()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \n",
            "For Num. of Epochs: 0, Time: 0.08222, Loss: 1.09322, Learning Rate: 0.01000, Error: 62.13141 %\n",
            "Test error: 54.74074 %\n",
            " \n",
            "For Num. of Epochs: 10, Time: 0.89152, Loss: 0.54099, Learning Rate: 0.01000, Error: 11.84295 %\n",
            "Test error: 11.96296 %\n",
            " \n",
            "For Num. of Epochs: 20, Time: 1.70519, Loss: 0.22454, Learning Rate: 0.00667, Error: 8.31731 %\n",
            "Test error: 8.25926 %\n",
            " \n",
            "For Num. of Epochs: 30, Time: 2.50954, Loss: 0.16676, Learning Rate: 0.00444, Error: 6.16987 %\n",
            "Test error: 5.55556 %\n",
            " \n",
            "For Num. of Epochs: 40, Time: 3.31022, Loss: 0.13404, Learning Rate: 0.00296, Error: 4.51923 %\n",
            "Test error: 4.11111 %\n",
            " \n",
            "For Num. of Epochs: 50, Time: 4.12123, Loss: 0.11918, Learning Rate: 0.00198, Error: 3.47756 %\n",
            "Test error: 3.11111 %\n",
            " \n",
            "For Num. of Epochs: 60, Time: 4.94557, Loss: 0.10768, Learning Rate: 0.00132, Error: 2.83654 %\n",
            "Test error: 2.44445 %\n",
            " \n",
            "For Num. of Epochs: 70, Time: 5.77425, Loss: 0.10098, Learning Rate: 0.00088, Error: 2.40385 %\n",
            "Test error: 2.00000 %\n",
            " \n",
            "For Num. of Epochs: 80, Time: 6.59813, Loss: 0.09679, Learning Rate: 0.00059, Error: 2.30769 %\n",
            "Test error: 2.11111 %\n",
            " \n",
            "For Num. of Epochs: 90, Time: 7.40386, Loss: 0.09439, Learning Rate: 0.00039, Error: 2.21154 %\n",
            "Test error: 2.11111 %\n",
            " \n",
            "For Num. of Epochs: 100, Time: 8.23059, Loss: 0.09293, Learning Rate: 0.00026, Error: 2.21154 %\n",
            "Test error: 1.77778 %\n",
            " \n",
            "For Num. of Epochs: 110, Time: 9.06046, Loss: 0.09158, Learning Rate: 0.00017, Error: 2.01923 %\n",
            "Test error: 1.77778 %\n",
            " \n",
            "For Num. of Epochs: 120, Time: 9.86889, Loss: 0.09071, Learning Rate: 0.00012, Error: 2.06731 %\n",
            "Test error: 1.77778 %\n",
            " \n",
            "For Num. of Epochs: 130, Time: 10.68032, Loss: 0.09047, Learning Rate: 0.00008, Error: 1.97115 %\n",
            "Test error: 1.77778 %\n",
            " \n",
            "For Num. of Epochs: 140, Time: 11.47928, Loss: 0.08976, Learning Rate: 0.00005, Error: 1.97115 %\n",
            "Test error: 1.77778 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R7My5gngk53m",
        "outputId": "1ece9805-944d-43f7-bdfd-7c33cf1c8415"
      },
      "source": [
        "# Print Classification Error on Test set\n",
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 150,     Time: 12.22337,     mini batch size: 20,     learning rate: 0.0001,     num of hidden nodes: 200\n",
            "Test error: 1.77778 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "juqlLruwxhJt"
      },
      "source": [
        "## Summary: \n",
        "### Final hyper parameters used: \n",
        "\n",
        "```\n",
        "  1.   Number of nodes of hidden layers = 200\n",
        "  2.   Number of epochs = 150\n",
        "  3.   Learning rates = 0.01 (Adaptive learning rate that divides by 1.5 every 10 epochs)\n",
        "  4.   Mini-batch size = 20\n",
        "```\n",
        "\n",
        "### Classification Error on Test set: \n",
        "    Test accuracy: 1.77778  %\n",
        "    Time: 12.22337 s\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3wdUHF5WsckH"
      },
      "source": [
        "# Task 2: \n",
        "You are asked to study the effect of network structure: hidden nodes, hidden layers to the classification performance. That is, you try different network configurations and understand the patterns. Your experiments have to be well-documented in your Jupyter notebook file and your report. It has to cover different aspects of network configurations such as shallow network, wide network, deep network etc."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdvQXcsLvBLP"
      },
      "source": [
        "## Changing the number of hidden nodes (using 3 layers)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZHcgPtBnsdbj"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 300  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CvND3jnLsdbl",
        "outputId": "0c35372f-6753-446c-ee81-728cb6b545e5"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.08204, Loss: 1.10103, Learning Rate: 0.00100, Error: 70.92902 %\n",
            "Test error: 70.16667 %\n",
            "For Num. of Epochs: 10, Time: 0.95110, Loss: 1.08510, Learning Rate: 0.00100, Error: 62.74991 %\n",
            "Test error: 61.98611 %\n",
            "For Num. of Epochs: 20, Time: 1.81506, Loss: 1.07195, Learning Rate: 0.00100, Error: 57.72204 %\n",
            "Test error: 56.76389 %\n",
            "For Num. of Epochs: 30, Time: 2.67028, Loss: 1.05834, Learning Rate: 0.00100, Error: 48.34262 %\n",
            "Test error: 48.66667 %\n",
            "For Num. of Epochs: 40, Time: 3.51786, Loss: 1.04181, Learning Rate: 0.00100, Error: 44.18803 %\n",
            "Test error: 43.77778 %\n",
            "For Num. of Epochs: 50, Time: 4.38073, Loss: 1.02070, Learning Rate: 0.00100, Error: 36.87105 %\n",
            "Test error: 38.56944 %\n",
            "For Num. of Epochs: 60, Time: 5.24633, Loss: 0.99285, Learning Rate: 0.00100, Error: 29.83278 %\n",
            "Test error: 30.72222 %\n",
            "For Num. of Epochs: 70, Time: 6.14356, Loss: 0.95584, Learning Rate: 0.00100, Error: 19.23077 %\n",
            "Test error: 17.87500 %\n",
            "For Num. of Epochs: 80, Time: 6.99840, Loss: 0.90779, Learning Rate: 0.00100, Error: 15.77852 %\n",
            "Test error: 15.20833 %\n",
            "For Num. of Epochs: 90, Time: 7.85760, Loss: 0.84786, Learning Rate: 0.00100, Error: 13.54144 %\n",
            "Test error: 13.77778 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rKF7qazkze2G",
        "outputId": "7c53e61d-ac0b-4ba4-bbb3-bb50edb7baf0"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.60161,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 33.05556 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cHuGiFQbv2US",
        "outputId": "1dcf2a15-e3d9-4e22-e7b4-ec59b3f810e3"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 5.05955,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 150\n",
            "Test error: 15.37500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_YeqnjahwfC9",
        "outputId": "2b8247e0-67e1-40c2-f037-c22c4b2bb238"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 5.73307,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 200\n",
            "Test error: 14.95833 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tbOJysBUv92q",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "793487f3-9798-4ff6-edaa-e24169adb12e"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 8.61859,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 300\n",
            "Test error: 12.66667 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XGiuXtZTCXlG",
        "outputId": "eeb1ac96-db72-42b0-9036-a7e063d1823e"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 11.00542,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 400\n",
            "Test error: 11.08333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SmJQgFSxztSN"
      },
      "source": [
        "## Results: \n",
        "### Constant hyper parameters used: \n",
        "\n",
        "  1.   Number of epochs = 100\n",
        "  2.   Learning rates = 0.001 (Fixed)\n",
        "  3.   Mini-batch size = 30\n",
        "\n",
        "\n",
        "### Classification Error on Test set for Number of Hidden Nodes: \n",
        "- num of hidden nodes: 100, Time: 4.60161, Test error: 33.05556 %\n",
        "- num of hidden nodes: 150, Time: 5.05955, Test error: 15.37500 %\n",
        "- num of hidden nodes: 200, Time: 5.73307, Test error: 14.95833 %\n",
        "- num of hidden nodes: 300, Time: 8.61859, Test error: 12.66667 %\n",
        "- num of hidden nodes: 400, Time: 11.00542, Test error: 11.08333 %\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wip71tsHuyVg"
      },
      "source": [
        "## Changing the number of hidden layers (using 100 hidden nodes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UXGnP7Dy3uUZ"
      },
      "source": [
        "### 1 Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IXCdSF-axpD_"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Fully connected neural network with 1 layer\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yPgtFOD-xpD_",
        "outputId": "630dba73-13fc-4965-f893-f8c98d2ab45c"
      },
      "source": [
        "model = FeedForward(input_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.03039, Loss: 1.12992, Learning Rate: 0.00100, Error: 65.67447 %\n",
            "Test error: 66.38889 %\n",
            "For Num. of Epochs: 10, Time: 0.32311, Loss: 1.05257, Learning Rate: 0.00100, Error: 50.69862 %\n",
            "Test error: 49.68055 %\n",
            "For Num. of Epochs: 20, Time: 0.61068, Loss: 1.02334, Learning Rate: 0.00100, Error: 39.58380 %\n",
            "Test error: 38.59722 %\n",
            "For Num. of Epochs: 30, Time: 0.90983, Loss: 0.99619, Learning Rate: 0.00100, Error: 33.77926 %\n",
            "Test error: 31.70833 %\n",
            "For Num. of Epochs: 40, Time: 1.19396, Loss: 0.97075, Learning Rate: 0.00100, Error: 29.57637 %\n",
            "Test error: 28.27778 %\n",
            "For Num. of Epochs: 50, Time: 1.46618, Loss: 0.94692, Learning Rate: 0.00100, Error: 26.44370 %\n",
            "Test error: 24.72222 %\n",
            "For Num. of Epochs: 60, Time: 1.74402, Loss: 0.92439, Learning Rate: 0.00100, Error: 25.22111 %\n",
            "Test error: 23.38889 %\n",
            "For Num. of Epochs: 70, Time: 2.02792, Loss: 0.90340, Learning Rate: 0.00100, Error: 23.95764 %\n",
            "Test error: 21.62500 %\n",
            "For Num. of Epochs: 80, Time: 2.30471, Loss: 0.88335, Learning Rate: 0.00100, Error: 22.25938 %\n",
            "Test error: 20.86111 %\n",
            "For Num. of Epochs: 90, Time: 2.58111, Loss: 0.86461, Learning Rate: 0.00100, Error: 21.53475 %\n",
            "Test error: 20.75000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qIYAWM7VxpEA",
        "outputId": "eb7cb878-7a4e-42b7-ffb7-5b27e54517db"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 2.83738,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 19.63889 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rppk9uyz39-w"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybS13WEE4KP2"
      },
      "source": [
        "### 2 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AstTt9f7x7KQ"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 2 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8d3EKD7px7KQ",
        "outputId": "9f2cbc8d-0f47-47fa-a61a-04956d8889af"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04402, Loss: 4.59894, Learning Rate: 0.00100, Error: 100.00000 %\n",
            "Test error: 99.45833 %\n",
            "For Num. of Epochs: 10, Time: 0.46247, Loss: 1.72215, Learning Rate: 0.00100, Error: 52.06986 %\n",
            "Test error: 54.30555 %\n",
            "For Num. of Epochs: 20, Time: 0.86610, Loss: 1.12875, Learning Rate: 0.00100, Error: 46.75585 %\n",
            "Test error: 51.63889 %\n",
            "For Num. of Epochs: 30, Time: 1.31397, Loss: 1.05840, Learning Rate: 0.00100, Error: 43.30360 %\n",
            "Test error: 43.88889 %\n",
            "For Num. of Epochs: 40, Time: 1.74355, Loss: 1.01801, Learning Rate: 0.00100, Error: 36.96024 %\n",
            "Test error: 36.34722 %\n",
            "For Num. of Epochs: 50, Time: 2.16157, Loss: 0.98182, Learning Rate: 0.00100, Error: 30.26013 %\n",
            "Test error: 30.37500 %\n",
            "For Num. of Epochs: 60, Time: 2.58490, Loss: 0.94540, Learning Rate: 0.00100, Error: 26.22817 %\n",
            "Test error: 24.37500 %\n",
            "For Num. of Epochs: 70, Time: 2.99616, Loss: 0.90687, Learning Rate: 0.00100, Error: 21.90635 %\n",
            "Test error: 20.27778 %\n",
            "For Num. of Epochs: 80, Time: 3.40633, Loss: 0.86699, Learning Rate: 0.00100, Error: 18.80342 %\n",
            "Test error: 19.05556 %\n",
            "For Num. of Epochs: 90, Time: 3.83085, Loss: 0.82580, Learning Rate: 0.00100, Error: 17.51765 %\n",
            "Test error: 17.94444 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_qV-U241x7KR",
        "outputId": "3bb9f4f5-ade8-4a89-93bc-6757b49416fa"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.19657,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 16.05556 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAaSttpF4LSK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c3v312qM2U_r"
      },
      "source": [
        "### 3 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JE1qAICx81t"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OGxDTnaZx81t",
        "outputId": "5e6237a3-3869-4ab7-8792-e3c44f74a01d"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04664, Loss: 1.09888, Learning Rate: 0.00100, Error: 63.85358 %\n",
            "Test error: 63.08333 %\n",
            "For Num. of Epochs: 10, Time: 0.52427, Loss: 1.08936, Learning Rate: 0.00100, Error: 63.87588 %\n",
            "Test error: 63.08333 %\n",
            "For Num. of Epochs: 20, Time: 0.97953, Loss: 1.08042, Learning Rate: 0.00100, Error: 63.77555 %\n",
            "Test error: 63.08333 %\n",
            "For Num. of Epochs: 30, Time: 1.44412, Loss: 1.07111, Learning Rate: 0.00100, Error: 57.56596 %\n",
            "Test error: 55.54167 %\n",
            "For Num. of Epochs: 40, Time: 1.91863, Loss: 1.06063, Learning Rate: 0.00100, Error: 45.61873 %\n",
            "Test error: 43.25000 %\n",
            "For Num. of Epochs: 50, Time: 2.38326, Loss: 1.04785, Learning Rate: 0.00100, Error: 40.39390 %\n",
            "Test error: 38.13889 %\n",
            "For Num. of Epochs: 60, Time: 2.83489, Loss: 1.03168, Learning Rate: 0.00100, Error: 36.37681 %\n",
            "Test error: 35.90278 %\n",
            "For Num. of Epochs: 70, Time: 3.31162, Loss: 1.01074, Learning Rate: 0.00100, Error: 32.65329 %\n",
            "Test error: 33.47222 %\n",
            "For Num. of Epochs: 80, Time: 3.76549, Loss: 0.98331, Learning Rate: 0.00100, Error: 30.14864 %\n",
            "Test error: 30.48611 %\n",
            "For Num. of Epochs: 90, Time: 4.24455, Loss: 0.94725, Learning Rate: 0.00100, Error: 25.55927 %\n",
            "Test error: 25.04167 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6_1ZgLF-x81u",
        "outputId": "464a6bc0-ee7b-45e6-b4b9-72455d2270e2"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.65410,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 17.38889 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4-ZxnM7N20lp"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NqHtC2lW4qI-"
      },
      "source": [
        "### 4 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qjbwtn6Bx-4f"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Fully connected neural network with 4 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc4 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc4(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cNkoNznRx-4f",
        "outputId": "82f472dc-b2bd-4f3b-c989-9e92e40bb242"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.05949, Loss: 1.10062, Learning Rate: 0.00100, Error: 68.85544 %\n",
            "Test error: 70.87500 %\n",
            "For Num. of Epochs: 10, Time: 0.63963, Loss: 1.09777, Learning Rate: 0.00100, Error: 62.06615 %\n",
            "Test error: 61.87500 %\n",
            "For Num. of Epochs: 20, Time: 1.19648, Loss: 1.09543, Learning Rate: 0.00100, Error: 62.51951 %\n",
            "Test error: 61.41667 %\n",
            "For Num. of Epochs: 30, Time: 1.76419, Loss: 1.09345, Learning Rate: 0.00100, Error: 61.52731 %\n",
            "Test error: 60.30555 %\n",
            "For Num. of Epochs: 40, Time: 2.32070, Loss: 1.09151, Learning Rate: 0.00100, Error: 59.11185 %\n",
            "Test error: 58.63889 %\n",
            "For Num. of Epochs: 50, Time: 2.88235, Loss: 1.08952, Learning Rate: 0.00100, Error: 56.72241 %\n",
            "Test error: 57.08333 %\n",
            "For Num. of Epochs: 60, Time: 3.43101, Loss: 1.08741, Learning Rate: 0.00100, Error: 54.61167 %\n",
            "Test error: 53.66667 %\n",
            "For Num. of Epochs: 70, Time: 3.99634, Loss: 1.08512, Learning Rate: 0.00100, Error: 50.78038 %\n",
            "Test error: 50.11111 %\n",
            "For Num. of Epochs: 80, Time: 4.55501, Loss: 1.08259, Learning Rate: 0.00100, Error: 47.06429 %\n",
            "Test error: 47.12500 %\n",
            "For Num. of Epochs: 90, Time: 5.10544, Loss: 1.07973, Learning Rate: 0.00100, Error: 44.80119 %\n",
            "Test error: 44.69444 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gE-ybcNgx-4f",
        "outputId": "dba20a02-6b54-4b3f-d3fc-bd4c7151c5a7"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 5.61082,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 43.34722 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GbFQVyfK4qca"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WxJoYQ_F2064"
      },
      "source": [
        "### 5 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z60F2B4giEDb"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Fully connected neural network with 5 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc4 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc5 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc4(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc5(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPJ46JPwiEDd",
        "outputId": "b92ae9ed-1650-47ef-90c2-62d8cc8f695f"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.07276, Loss: 1.09826, Learning Rate: 0.00100, Error: 64.11371 %\n",
            "Test error: 64.16667 %\n",
            "For Num. of Epochs: 10, Time: 0.70652, Loss: 1.09800, Learning Rate: 0.00100, Error: 63.45968 %\n",
            "Test error: 63.50000 %\n",
            "For Num. of Epochs: 20, Time: 1.32923, Loss: 1.09772, Learning Rate: 0.00100, Error: 63.03976 %\n",
            "Test error: 62.38889 %\n",
            "For Num. of Epochs: 30, Time: 1.94784, Loss: 1.09744, Learning Rate: 0.00100, Error: 61.89521 %\n",
            "Test error: 61.94444 %\n",
            "For Num. of Epochs: 40, Time: 2.60291, Loss: 1.09714, Learning Rate: 0.00100, Error: 60.40134 %\n",
            "Test error: 60.70833 %\n",
            "For Num. of Epochs: 50, Time: 3.24247, Loss: 1.09684, Learning Rate: 0.00100, Error: 58.70308 %\n",
            "Test error: 59.16667 %\n",
            "For Num. of Epochs: 60, Time: 3.88233, Loss: 1.09650, Learning Rate: 0.00100, Error: 57.48049 %\n",
            "Test error: 58.94444 %\n",
            "For Num. of Epochs: 70, Time: 4.49720, Loss: 1.09614, Learning Rate: 0.00100, Error: 56.47715 %\n",
            "Test error: 57.06944 %\n",
            "For Num. of Epochs: 80, Time: 5.11447, Loss: 1.09575, Learning Rate: 0.00100, Error: 55.60386 %\n",
            "Test error: 57.27778 %\n",
            "For Num. of Epochs: 90, Time: 5.72688, Loss: 1.09533, Learning Rate: 0.00100, Error: 55.75622 %\n",
            "Test error: 57.08333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m141L966iEDg",
        "outputId": "fb85d8c4-58a9-4b1d-d852-6c21682c9998"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 6.28459,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 56.63889 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ntRQuufHiDqP"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EvtiHOeAiJqk"
      },
      "source": [
        "## Increasing the learning rate to allow nnet to train faster"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9DedH7ekEeX"
      },
      "source": [
        "### 1 Layer"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ooTsHvrCkEeY"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Fully connected neural network with 1 layer\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M_pIvlpzkEeY",
        "outputId": "1c564c46-0ddc-4688-c780-5d66f3db78e8"
      },
      "source": [
        "model = FeedForward(input_size, output_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.02961, Loss: 1.07539, Learning Rate: 0.01000, Error: 52.12560 %\n",
            "Test error: 48.97222 %\n",
            "For Num. of Epochs: 10, Time: 0.31884, Loss: 0.83949, Learning Rate: 0.01000, Error: 19.64326 %\n",
            "Test error: 16.73611 %\n",
            "For Num. of Epochs: 20, Time: 0.60527, Loss: 0.70700, Learning Rate: 0.01000, Error: 12.72761 %\n",
            "Test error: 11.52778 %\n",
            "For Num. of Epochs: 30, Time: 0.88669, Loss: 0.62224, Learning Rate: 0.01000, Error: 10.17466 %\n",
            "Test error: 11.19445 %\n",
            "For Num. of Epochs: 40, Time: 1.17868, Loss: 0.56231, Learning Rate: 0.01000, Error: 9.78818 %\n",
            "Test error: 9.76389 %\n",
            "For Num. of Epochs: 50, Time: 1.46798, Loss: 0.51789, Learning Rate: 0.01000, Error: 8.75883 %\n",
            "Test error: 8.09722 %\n",
            "For Num. of Epochs: 60, Time: 1.75185, Loss: 0.48325, Learning Rate: 0.01000, Error: 8.13824 %\n",
            "Test error: 7.98611 %\n",
            "For Num. of Epochs: 70, Time: 2.02709, Loss: 0.45482, Learning Rate: 0.01000, Error: 7.79264 %\n",
            "Test error: 7.98611 %\n",
            "For Num. of Epochs: 80, Time: 2.29686, Loss: 0.43142, Learning Rate: 0.01000, Error: 7.80751 %\n",
            "Test error: 7.54167 %\n",
            "For Num. of Epochs: 90, Time: 2.58477, Loss: 0.41131, Learning Rate: 0.01000, Error: 7.59197 %\n",
            "Test error: 7.31945 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Gk3cd6XGkEeY",
        "outputId": "2c30d9d4-33e6-4294-945a-d6866cba7e7f"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 2.83572,     mini batch size: 30,     learning rate: 0.0100,     num of hidden nodes: 100\n",
            "Test error: 7.09722 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NzWqnNBVkEeZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N-y9CO2xkEeZ"
      },
      "source": [
        "### 2 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xfgC4pUBkEeZ"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Make a fully connected neural network with 2 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TjgsgGOYkEeZ",
        "outputId": "ecfe27ec-e55f-4d87-ee52-305887e777a9"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04440, Loss: 3.89981, Learning Rate: 0.01000, Error: 71.89892 %\n",
            "Test error: 66.38889 %\n",
            "For Num. of Epochs: 10, Time: 0.47811, Loss: 0.83930, Learning Rate: 0.01000, Error: 19.12672 %\n",
            "Test error: 19.06945 %\n",
            "For Num. of Epochs: 20, Time: 0.90281, Loss: 0.50557, Learning Rate: 0.01000, Error: 7.39874 %\n",
            "Test error: 7.31945 %\n",
            "For Num. of Epochs: 30, Time: 1.33247, Loss: 0.34587, Learning Rate: 0.01000, Error: 5.86771 %\n",
            "Test error: 5.44445 %\n",
            "For Num. of Epochs: 40, Time: 1.76552, Loss: 0.26461, Learning Rate: 0.01000, Error: 4.99814 %\n",
            "Test error: 5.00000 %\n",
            "For Num. of Epochs: 50, Time: 2.20631, Loss: 0.21472, Learning Rate: 0.01000, Error: 4.54850 %\n",
            "Test error: 4.55556 %\n",
            "For Num. of Epochs: 60, Time: 2.62222, Loss: 0.18136, Learning Rate: 0.01000, Error: 4.39614 %\n",
            "Test error: 4.22222 %\n",
            "For Num. of Epochs: 70, Time: 3.04918, Loss: 0.15671, Learning Rate: 0.01000, Error: 3.72724 %\n",
            "Test error: 3.44445 %\n",
            "For Num. of Epochs: 80, Time: 3.45664, Loss: 0.13836, Learning Rate: 0.01000, Error: 3.58231 %\n",
            "Test error: 2.77778 %\n",
            "For Num. of Epochs: 90, Time: 3.86849, Loss: 0.12353, Learning Rate: 0.01000, Error: 3.15496 %\n",
            "Test error: 2.33333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PMBoAec9kEeZ",
        "outputId": "5f2f0ec5-b807-49ad-eec1-4896a7947c7f"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.24768,     mini batch size: 30,     learning rate: 0.0100,     num of hidden nodes: 100\n",
            "Test error: 2.55556 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XK4xnWKwkEea"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JhLjKV4VkEea"
      },
      "source": [
        "### 3 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "awBi68yzkEea"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yTJ9KDKkkEea",
        "outputId": "1c43b885-f316-4e4f-a2b8-e0a9725b9c5d"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04598, Loss: 1.09874, Learning Rate: 0.01000, Error: 58.92977 %\n",
            "Test error: 50.76389 %\n",
            "For Num. of Epochs: 10, Time: 0.50619, Loss: 0.95326, Learning Rate: 0.01000, Error: 24.65255 %\n",
            "Test error: 21.69445 %\n",
            "For Num. of Epochs: 20, Time: 0.94559, Loss: 0.42931, Learning Rate: 0.01000, Error: 12.45262 %\n",
            "Test error: 13.65278 %\n",
            "For Num. of Epochs: 30, Time: 1.41122, Loss: 0.25533, Learning Rate: 0.01000, Error: 10.25641 %\n",
            "Test error: 8.87500 %\n",
            "For Num. of Epochs: 40, Time: 1.87390, Loss: 0.19476, Learning Rate: 0.01000, Error: 7.65515 %\n",
            "Test error: 7.98611 %\n",
            "For Num. of Epochs: 50, Time: 2.34801, Loss: 0.15796, Learning Rate: 0.01000, Error: 6.23189 %\n",
            "Test error: 6.22222 %\n",
            "For Num. of Epochs: 60, Time: 2.80837, Loss: 0.12593, Learning Rate: 0.01000, Error: 4.54106 %\n",
            "Test error: 4.44445 %\n",
            "For Num. of Epochs: 70, Time: 3.25938, Loss: 0.10083, Learning Rate: 0.01000, Error: 2.89855 %\n",
            "Test error: 2.77778 %\n",
            "For Num. of Epochs: 80, Time: 3.72749, Loss: 0.07658, Learning Rate: 0.01000, Error: 1.44928 %\n",
            "Test error: 1.44444 %\n",
            "For Num. of Epochs: 90, Time: 4.19582, Loss: 0.05973, Learning Rate: 0.01000, Error: 1.01449 %\n",
            "Test error: 1.00000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wz-zKuLAkEea",
        "outputId": "65dc9db6-5547-48f6-cbd7-9adf5fcbe092"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.61948,     mini batch size: 30,     learning rate: 0.0100,     num of hidden nodes: 100\n",
            "Test error: 0.66667 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_QsaUxrSkEeb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p0hbsaCckEeb"
      },
      "source": [
        "### 4 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XxgBmA5AkEeb"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Fully connected neural network with 4 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc4 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc4(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qRxWQiklkEeb",
        "outputId": "77519e65-5439-426a-bc6b-7aa07879cc94"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.05895, Loss: 1.09827, Learning Rate: 0.01000, Error: 65.54441 %\n",
            "Test error: 65.94444 %\n",
            "For Num. of Epochs: 10, Time: 0.63098, Loss: 1.08483, Learning Rate: 0.01000, Error: 65.28800 %\n",
            "Test error: 65.94444 %\n",
            "For Num. of Epochs: 20, Time: 1.16722, Loss: 1.00373, Learning Rate: 0.01000, Error: 58.19398 %\n",
            "Test error: 56.75000 %\n",
            "For Num. of Epochs: 30, Time: 1.71417, Loss: 0.43954, Learning Rate: 0.01000, Error: 11.92865 %\n",
            "Test error: 12.63889 %\n",
            "For Num. of Epochs: 40, Time: 2.25367, Loss: 0.21979, Learning Rate: 0.01000, Error: 8.93720 %\n",
            "Test error: 8.41667 %\n",
            "For Num. of Epochs: 50, Time: 2.79677, Loss: 0.16976, Learning Rate: 0.01000, Error: 7.36529 %\n",
            "Test error: 8.22222 %\n",
            "For Num. of Epochs: 60, Time: 3.34711, Loss: 0.12793, Learning Rate: 0.01000, Error: 5.65217 %\n",
            "Test error: 4.98611 %\n",
            "For Num. of Epochs: 70, Time: 3.88941, Loss: 0.10036, Learning Rate: 0.01000, Error: 3.81643 %\n",
            "Test error: 3.22222 %\n",
            "For Num. of Epochs: 80, Time: 4.42515, Loss: 0.06857, Learning Rate: 0.01000, Error: 1.94723 %\n",
            "Test error: 3.88889 %\n",
            "For Num. of Epochs: 90, Time: 4.99391, Loss: 0.04644, Learning Rate: 0.01000, Error: 0.91787 %\n",
            "Test error: 1.77778 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gDtGpqxAkEeb",
        "outputId": "a9c15056-4301-435e-c031-56c4df5a8ce6"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 5.48521,     mini batch size: 30,     learning rate: 0.0100,     num of hidden nodes: 100\n",
            "Test error: 0.33333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vFzLhUDkEec"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pcypnqzlkEec"
      },
      "source": [
        "### 5 Layers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KEhvi-rTkEec"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.01\n",
        "\n",
        "# Fully connected neural network with 5 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc4 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc5 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc4(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc5(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4SbJN6jkEec",
        "outputId": "1f61b1ae-e36f-4885-d6b1-89ad471f0b1d"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.06032, Loss: 1.09798, Learning Rate: 0.01000, Error: 63.83872 %\n",
            "Test error: 63.08333 %\n",
            "For Num. of Epochs: 10, Time: 0.68933, Loss: 1.09094, Learning Rate: 0.01000, Error: 63.82386 %\n",
            "Test error: 63.08333 %\n",
            "For Num. of Epochs: 20, Time: 1.33754, Loss: 1.05937, Learning Rate: 0.01000, Error: 44.13972 %\n",
            "Test error: 41.54167 %\n",
            "For Num. of Epochs: 30, Time: 1.97048, Loss: 0.49786, Learning Rate: 0.01000, Error: 14.00595 %\n",
            "Test error: 12.44445 %\n",
            "For Num. of Epochs: 40, Time: 2.59659, Loss: 0.21819, Learning Rate: 0.01000, Error: 8.99294 %\n",
            "Test error: 8.63889 %\n",
            "For Num. of Epochs: 50, Time: 3.25169, Loss: 0.16150, Learning Rate: 0.01000, Error: 6.52917 %\n",
            "Test error: 13.33333 %\n",
            "For Num. of Epochs: 60, Time: 3.87615, Loss: 0.10754, Learning Rate: 0.01000, Error: 3.83129 %\n",
            "Test error: 5.88889 %\n",
            "For Num. of Epochs: 70, Time: 4.49859, Loss: 0.07830, Learning Rate: 0.01000, Error: 2.68673 %\n",
            "Test error: 18.66667 %\n",
            "For Num. of Epochs: 80, Time: 5.13337, Loss: 0.04336, Learning Rate: 0.01000, Error: 1.01449 %\n",
            "Test error: 0.44444 %\n",
            "For Num. of Epochs: 90, Time: 5.77451, Loss: 0.02212, Learning Rate: 0.01000, Error: 0.33816 %\n",
            "Test error: 0.00000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8rq37H4YkEed",
        "outputId": "9dfb8ff2-38c6-4323-b38b-67281d33890b"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 6.34779,     mini batch size: 30,     learning rate: 0.0100,     num of hidden nodes: 100\n",
            "Test error: 0.00000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n3yNaGq1sc6O"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tu8ZcqII2Kqn"
      },
      "source": [
        "## Results: \n",
        "### Constant hyper parameters used: \n",
        "\n",
        "  1.   Number of epochs = 100\n",
        "  2.   Learning rates = 0.001 (Fixed)\n",
        "  3.   Mini-batch size = 30\n",
        "  4.   Number of hidden nodes = 100\n",
        "\n",
        "### Classification Error on Test set for Number of Layers: \n",
        "\n",
        "For learning rate = 0.001\n",
        "- num of layers: 1, Time: 2.83738, Test error: 19.63889 %\n",
        "- num of layers: 2, Time: 4.19657, Test error: 16.05556 %\n",
        "- num of layers: 3, Time: 4.65410, Test error: 17.38889 %\n",
        "- num of layers: 4, Time: 5.61082, Test error: 43.34722 %\n",
        "- num of layers: 5, Time: 6.28459, Test error: 56.63889 %\n",
        "\n",
        "For learning rate = 0.01\n",
        "- num of layers: 1, Time: 2.83572, Test error: 7.09722 %\n",
        "- num of layers: 2, Time: 4.24768, Test error: 2.55556 %\n",
        "- num of layers: 3, Time: 4.61948, Test error: 0.66667 %\n",
        "- num of layers: 4, Time: 5.48521, Test error: 0.33333 %\n",
        "- num of layers: 5, Time: 6.34779, Test error: 0.00000 %\n",
        "*NOTE: signs of overfitting in layers 4 and 5. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "so1nTa0fuBmV"
      },
      "source": [
        "## Shallow, Wide and Deep Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TfIVFrZbuOY_"
      },
      "source": [
        "### Shallow Neural Network\n",
        "1 hidden layer, 100 hidden nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9vWHFEB7uhQ1"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uUSt0bZ5uhQ1",
        "outputId": "dcfa46c6-9e75-4918-82fd-07ce7508e881"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04393, Loss: 4.57576, Learning Rate: 0.00100, Error: 100.00000 %\n",
            "Test error: 100.00000 %\n",
            "For Num. of Epochs: 10, Time: 0.50470, Loss: 2.13814, Learning Rate: 0.00100, Error: 61.81717 %\n",
            "Test error: 61.08333 %\n",
            "For Num. of Epochs: 20, Time: 0.91285, Loss: 1.15120, Learning Rate: 0.00100, Error: 48.33147 %\n",
            "Test error: 51.08333 %\n",
            "For Num. of Epochs: 30, Time: 1.33616, Loss: 1.06979, Learning Rate: 0.00100, Error: 40.89558 %\n",
            "Test error: 39.77778 %\n",
            "For Num. of Epochs: 40, Time: 1.75101, Loss: 1.02867, Learning Rate: 0.00100, Error: 30.30844 %\n",
            "Test error: 31.22222 %\n",
            "For Num. of Epochs: 50, Time: 2.18022, Loss: 0.99414, Learning Rate: 0.00100, Error: 26.75214 %\n",
            "Test error: 22.95833 %\n",
            "For Num. of Epochs: 60, Time: 2.60200, Loss: 0.95979, Learning Rate: 0.00100, Error: 22.71646 %\n",
            "Test error: 20.61111 %\n",
            "For Num. of Epochs: 70, Time: 3.03094, Loss: 0.92372, Learning Rate: 0.00100, Error: 22.98402 %\n",
            "Test error: 21.38889 %\n",
            "For Num. of Epochs: 80, Time: 3.45305, Loss: 0.88527, Learning Rate: 0.00100, Error: 18.81085 %\n",
            "Test error: 19.27778 %\n",
            "For Num. of Epochs: 90, Time: 3.87214, Loss: 0.84454, Learning Rate: 0.00100, Error: 16.99368 %\n",
            "Test error: 18.29167 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vBr7vOdquhQ2",
        "outputId": "31eaafbd-a1b9-4f69-80ed-b70db1e15d93"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.26615,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 15.51389 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yjxC1P-2ulhv"
      },
      "source": [
        "### Wide Neural Network\n",
        "1 Hidden Layer, 400 Hidden Nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_Hn9Swz6u2WT"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 400  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1NhDhd6u2WU",
        "outputId": "fee971d3-5644-4f02-e41a-c9e1c125287a"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.11435, Loss: 1.10392, Learning Rate: 0.00100, Error: 70.49424 %\n",
            "Test error: 70.41667 %\n",
            "For Num. of Epochs: 10, Time: 1.24530, Loss: 1.07794, Learning Rate: 0.00100, Error: 49.94797 %\n",
            "Test error: 49.00000 %\n",
            "For Num. of Epochs: 20, Time: 2.36674, Loss: 1.05880, Learning Rate: 0.00100, Error: 41.11483 %\n",
            "Test error: 39.69444 %\n",
            "For Num. of Epochs: 30, Time: 3.45325, Loss: 1.03730, Learning Rate: 0.00100, Error: 35.52583 %\n",
            "Test error: 37.22222 %\n",
            "For Num. of Epochs: 40, Time: 4.53970, Loss: 1.00971, Learning Rate: 0.00100, Error: 30.81011 %\n",
            "Test error: 31.45833 %\n",
            "For Num. of Epochs: 50, Time: 5.65794, Loss: 0.97369, Learning Rate: 0.00100, Error: 24.20661 %\n",
            "Test error: 25.15278 %\n",
            "For Num. of Epochs: 60, Time: 6.78702, Loss: 0.92615, Learning Rate: 0.00100, Error: 18.08993 %\n",
            "Test error: 16.50000 %\n",
            "For Num. of Epochs: 70, Time: 7.90686, Loss: 0.86582, Learning Rate: 0.00100, Error: 13.32219 %\n",
            "Test error: 12.62500 %\n",
            "For Num. of Epochs: 80, Time: 9.00503, Loss: 0.79459, Learning Rate: 0.00100, Error: 11.16685 %\n",
            "Test error: 10.09722 %\n",
            "For Num. of Epochs: 90, Time: 10.11904, Loss: 0.71851, Learning Rate: 0.00100, Error: 9.43516 %\n",
            "Test error: 9.43056 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hN9iMuaMu91B",
        "outputId": "d5c70d21-2655-43c9-f2fe-d9739d2552d2"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 11.11809,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 400\n",
            "Test error: 8.98611 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0R5ou6zRuL8n"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zUQmkJRRu_mZ"
      },
      "source": [
        "### Deep Neural Network\n",
        "3 Hidden Layers and 100 Hidden Nodes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OjS7VC-6vKS7"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 1000\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Fully connected neural network with 5 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc4 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc5 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc4(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc5(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pP8tAtznvKS7",
        "outputId": "42a873a5-3740-4d92-df0c-d7202b26b1a4"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 100 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 100 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.06783, Loss: 1.09900, Learning Rate: 0.00100, Error: 66.72612 %\n",
            "Test error: 66.20833 %\n",
            "For Num. of Epochs: 100, Time: 6.08217, Loss: 1.09450, Learning Rate: 0.00100, Error: 60.90673 %\n",
            "Test error: 60.86111 %\n",
            "For Num. of Epochs: 200, Time: 12.08068, Loss: 1.08387, Learning Rate: 0.00100, Error: 65.71163 %\n",
            "Test error: 66.38889 %\n",
            "For Num. of Epochs: 300, Time: 18.09626, Loss: 1.00164, Learning Rate: 0.00100, Error: 65.67447 %\n",
            "Test error: 66.38889 %\n",
            "For Num. of Epochs: 400, Time: 24.06647, Loss: 0.31375, Learning Rate: 0.00100, Error: 10.65032 %\n",
            "Test error: 10.65278 %\n",
            "For Num. of Epochs: 500, Time: 30.23740, Loss: 0.17204, Learning Rate: 0.00100, Error: 7.26124 %\n",
            "Test error: 7.20833 %\n",
            "For Num. of Epochs: 600, Time: 36.33602, Loss: 0.10802, Learning Rate: 0.00100, Error: 4.49275 %\n",
            "Test error: 4.33333 %\n",
            "For Num. of Epochs: 700, Time: 42.28646, Loss: 0.05968, Learning Rate: 0.00100, Error: 1.31178 %\n",
            "Test error: 1.22222 %\n",
            "For Num. of Epochs: 800, Time: 48.20439, Loss: 0.03408, Learning Rate: 0.00100, Error: 0.38647 %\n",
            "Test error: 0.22222 %\n",
            "For Num. of Epochs: 900, Time: 54.15468, Loss: 0.02147, Learning Rate: 0.00100, Error: 0.14493 %\n",
            "Test error: 0.00000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "btRpWm0FvKS8",
        "outputId": "024af214-236e-4677-ba64-72a64aedb89b"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 1000,     Time: 60.06633,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 0.00000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "74hP-0NlvARe"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaeqM0J1v0MR"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Constant hyper parameters used: \n",
        "  1.   Number of epochs = 100\n",
        "  2.   Learning rates = 0.001 (Fixed)\n",
        "  3.   Mini-batch size = 30\n",
        "\n",
        "### Shallow Neural Network:\n",
        "- 2 hidden layers\n",
        "- 100 hidden nodes\n",
        "- Time: 4.26615\n",
        "- Test error: 15.51389 %\n",
        "\n",
        "### Wide Neural Network:\n",
        "- 2 hidden layers\n",
        "- 400 hidden nodes\n",
        "- Time: 11.11809\n",
        "- Test error: 8.98611 %\n",
        "\n",
        "### Deep Neural Network:\n",
        "- 4 hidden layers\n",
        "- 200 hidden nodes\n",
        "- Time: 6.18328\n",
        "- Test error: 59.20833 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_8SP7sODk6J9"
      },
      "source": [
        "# Task 3: \n",
        "You are asked to study the effect of learning rates. As with Task 2, your experiments have to be well documented. You need to give correct conclusion and give suggestion how learning rates should be set. This includes possible adaptive learning rates where the value increases or decreases as the increase of epochs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DyZcF2BNp1jm"
      },
      "source": [
        "## Changing the learning rates"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-hj7EkJb1EI6"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Th08FO1y1EI6",
        "outputId": "66fb2497-210c-43d0-8c68-a1de035a918c"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04272, Loss: 1.09841, Learning Rate: 0.00100, Error: 70.59457 %\n",
            "Test error: 70.75000 %\n",
            "For Num. of Epochs: 10, Time: 0.50386, Loss: 1.08973, Learning Rate: 0.00100, Error: 58.61390 %\n",
            "Test error: 59.11111 %\n",
            "For Num. of Epochs: 20, Time: 0.94285, Loss: 1.08186, Learning Rate: 0.00100, Error: 50.96618 %\n",
            "Test error: 51.58333 %\n",
            "For Num. of Epochs: 30, Time: 1.40290, Loss: 1.07357, Learning Rate: 0.00100, Error: 43.86473 %\n",
            "Test error: 44.47222 %\n",
            "For Num. of Epochs: 40, Time: 1.88326, Loss: 1.06418, Learning Rate: 0.00100, Error: 44.88666 %\n",
            "Test error: 46.44444 %\n",
            "For Num. of Epochs: 50, Time: 2.36523, Loss: 1.05287, Learning Rate: 0.00100, Error: 45.31401 %\n",
            "Test error: 46.00000 %\n",
            "For Num. of Epochs: 60, Time: 2.81724, Loss: 1.03862, Learning Rate: 0.00100, Error: 43.21442 %\n",
            "Test error: 43.44444 %\n",
            "For Num. of Epochs: 70, Time: 3.28719, Loss: 1.01985, Learning Rate: 0.00100, Error: 40.68004 %\n",
            "Test error: 41.33333 %\n",
            "For Num. of Epochs: 80, Time: 3.74420, Loss: 0.99488, Learning Rate: 0.00100, Error: 35.70420 %\n",
            "Test error: 35.13889 %\n",
            "For Num. of Epochs: 90, Time: 4.19498, Loss: 0.96157, Learning Rate: 0.00100, Error: 26.22074 %\n",
            "Test error: 24.37500 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQ5LZmNm1EI7",
        "outputId": "fdbd361b-dfef-4db7-a03f-0493cd258bc5"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.62454,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 17.61111 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0D0q989a1fRF",
        "outputId": "cfb42998-82bb-44e0-e8dd-03fac333530f"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.64185,     mini batch size: 30,     learning rate: 0.0030,     num of hidden nodes: 100\n",
            "Test error: 7.09722 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cUW2Qekc1fcH",
        "outputId": "f6b3f0a3-a7d9-40e2-f64e-6388ef6ef56b"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.72465,     mini batch size: 30,     learning rate: 0.0050,     num of hidden nodes: 100\n",
            "Test error: 6.20833 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MD3fzYXE1fgd",
        "outputId": "f02633d4-b01c-4a90-b12c-74318918b8bb"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.84306,     mini batch size: 30,     learning rate: 0.0070,     num of hidden nodes: 100\n",
            "Test error: 2.22222 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YOB4R9Ta1hNL",
        "outputId": "01aee76b-b2f6-4e22-dc18-39b087c4b915"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.69736,     mini batch size: 30,     learning rate: 0.0090,     num of hidden nodes: 100\n",
            "Test error: 0.77778 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-iULX7Do1hkb"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NiY4_79y-rMM"
      },
      "source": [
        "## Results: \n",
        "### Constant hyper parameters used: \n",
        "\n",
        "  1.   Number of epochs = 100\n",
        "  2.   Mini-batch size = 30\n",
        "  3.   Number of hidden nodes = 100\n",
        "\n",
        "\n",
        "### Classification Error on Test set for Fixed Learning Rates: \n",
        "- learning rate: 0.0010, Time: 4.62454, Test error: 17.61111 %\n",
        "- learning rate: 0.0030, Time: 4.64185, Test error: 7.09722 %\n",
        "- learning rate: 0.0050, Time: 4.72465, Test error: 6.20833 %\n",
        "- learning rate: 0.0070, Time: 4.84306, Test error: 2.22222 %\n",
        "- learning rate: 0.0090, Time: 4.69736, Test error: 0.77778 %\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lzwM6ztQpiCJ"
      },
      "source": [
        "## Adaptive learning rates\n",
        "Divide the learning rate by `x` every 10 epochs\n",
        "- when x > 1, learning rate decreases with the number of epochs\n",
        "- when x < 1, learning rate increases with the number of epochs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "drRkQoHx6lxd"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.005\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OpgmCgLmBCQF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2065dce0-b40f-4f93-9a3a-1c3d1fbaee3e"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    # learning rate strategy : divide the learning rate by x every 10 epochs\n",
        "    x = 1.5   # Change this\n",
        "    if epoch % 10 == 0 and epoch > 10: \n",
        "        learning_rate = learning_rate / x\n",
        "    \n",
        "    # create a new optimizer at the beginning of each epoch: give the current learning rate.   \n",
        "    optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "        \n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "        \n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\"\n",
        "    # by the number of batches\n",
        "    \n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats \n",
        "    # and compute the error rate on the test set  \n",
        "    \n",
        "    if epoch % 10 == 0 : \n",
        "            \n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        \n",
        "        eval_on_test_set()\n",
        "\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.04356, Loss: 1.09902, Learning Rate: 0.00500, Error: 66.05351 %\n",
            "Test error: 63.09722 %\n",
            "For Num. of Epochs: 10, Time: 0.50549, Loss: 1.07140, Learning Rate: 0.00500, Error: 50.61315 %\n",
            "Test error: 49.68055 %\n",
            "For Num. of Epochs: 20, Time: 0.95956, Loss: 1.01333, Learning Rate: 0.00333, Error: 43.04348 %\n",
            "Test error: 43.34722 %\n",
            "For Num. of Epochs: 30, Time: 1.42415, Loss: 0.90432, Learning Rate: 0.00222, Error: 25.49981 %\n",
            "Test error: 23.13889 %\n",
            "For Num. of Epochs: 40, Time: 1.88610, Loss: 0.77574, Learning Rate: 0.00148, Error: 16.57748 %\n",
            "Test error: 14.16667 %\n",
            "For Num. of Epochs: 50, Time: 2.35708, Loss: 0.67903, Learning Rate: 0.00099, Error: 13.88703 %\n",
            "Test error: 12.62500 %\n",
            "For Num. of Epochs: 60, Time: 2.81721, Loss: 0.61837, Learning Rate: 0.00066, Error: 13.41137 %\n",
            "Test error: 12.06944 %\n",
            "For Num. of Epochs: 70, Time: 3.26433, Loss: 0.58085, Learning Rate: 0.00044, Error: 12.67187 %\n",
            "Test error: 11.86111 %\n",
            "For Num. of Epochs: 80, Time: 3.73191, Loss: 0.55744, Learning Rate: 0.00029, Error: 12.33371 %\n",
            "Test error: 12.29167 %\n",
            "For Num. of Epochs: 90, Time: 4.19728, Loss: 0.54257, Learning Rate: 0.00020, Error: 11.96210 %\n",
            "Test error: 12.06945 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XtJ5nR2QJR-2"
      },
      "source": [
        "Starting Learning Rate: 0.01"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-ePNeP6K8Ma4",
        "outputId": "0dcba7ad-399f-4fe0-ac16-a1bf4866e1d6"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.61761,     mini batch size: 30,     x: 1.3,     ending learning rate: 0.0012,     num of hidden nodes: 100\n",
            "Test error: 4.22222 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fcC2KmFC7D8P",
        "outputId": "4c9a6718-8176-4803-9f68-a35e4d3d0ebb"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.66165,     mini batch size: 30,     x: 1.4,     ending learning rate: 0.0007,     num of hidden nodes: 100\n",
            "Test error: 6.22222 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dbHCnH5d8Nu_",
        "outputId": "197d8d51-c3dc-46ee-9eae-723eff22db6b"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.65058,     mini batch size: 30,     x: 1.5,     ending learning rate: 0.0004,     num of hidden nodes: 100\n",
            "Test error: 7.20833 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pDWlNmcuJVRY"
      },
      "source": [
        "Starting Learning Rate: 0.007"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FfKTu5hSJVRZ",
        "outputId": "35652362-de6f-47d8-ef6a-51151cf24812"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.52604,     mini batch size: 30,     x: 1.3,     ending learning rate: 0.0009,     num of hidden nodes: 100\n",
            "Test error: 7.76389 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uP-sXsm4JVRZ",
        "outputId": "1418d6b7-bfeb-44be-8433-4363721b9f73"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.63525,     mini batch size: 30,     x: 1.4,     ending learning rate: 0.0005,     num of hidden nodes: 100\n",
            "Test error: 8.54167 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-hsVhlgnJVRZ",
        "outputId": "aa864f59-ba36-4f9e-dfad-c8617e717c19"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.79620,     mini batch size: 30,     x: 1.5,     ending learning rate: 0.0003,     num of hidden nodes: 100\n",
            "Test error: 10.08333 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ld9CLqoLJM5r"
      },
      "source": [
        "Starting Learning Rate: 0.005"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_wzHgc3on518",
        "outputId": "62cdec94-f9f7-485f-afd3-43cec45a351a"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.64237,     mini batch size: 30,     x: 0.8,     ending learning rate: 0.0298,     num of hidden nodes: 100\n",
            "Test error: 1.22222 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5tqiLTAZmuoj",
        "outputId": "dfdc2774-12d6-41cb-81f4-1994c39bacf4"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.65208,     mini batch size: 30,     x: 0.9,     ending learning rate: 0.0116,     num of hidden nodes: 100\n",
            "Test error: 1.77778 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "spdF10e_GkMU",
        "outputId": "0f8b4c4c-65eb-4186-929d-7d7b46edbda0"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.56385,     mini batch size: 30,     x: 1.3,     ending learning rate: 0.0006,     num of hidden nodes: 100\n",
            "Test error: 8.20833 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YQvja3IrGkMV",
        "outputId": "693cc143-4ea2-48e6-a4c9-deae94a77ac9"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.64512,     mini batch size: 30,     x: 1.4,     ending learning rate: 0.0003,     num of hidden nodes: 100\n",
            "Test error: 10.09722 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xkFidyADGkMV",
        "outputId": "8bd3b4a1-7a35-41d8-a8b4-bb353b0e24e5"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.61243,     mini batch size: 30,     x: 1.5,     ending learning rate: 0.0002,     num of hidden nodes: 100\n",
            "Test error: 12.06945 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-DiRDzrsnCPl"
      },
      "source": [
        "Starting Learning Rate: 0.003"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZWNw8yUKnIw2",
        "outputId": "ed7c4b0f-1cfb-4792-bb09-552ce4ad4b77"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.60890,     mini batch size: 30,     x: 0.8,     ending learning rate: 0.0179,     num of hidden nodes: 100\n",
            "Test error: 3.44445 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qw9xsMTAnIw1",
        "outputId": "ade9f434-0e03-4ae8-e228-1d878f3c944d"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    x: {:.1f}, \\\n",
        "    ending learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, x, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.66232,     mini batch size: 30,     x: 0.9,     ending learning rate: 0.0070,     num of hidden nodes: 100\n",
            "Test error: 5.00000 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BMvd-D1enIaR"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKYTFPwA8zmL"
      },
      "source": [
        "## Results: \n",
        "### Constant hyper parameters used: \n",
        "\n",
        "  1.   Number of epochs = 100\n",
        "  2.   Mini-batch size = 30\n",
        "  3.   Number of hidden nodes = 100\n",
        "\n",
        "Divide the learning rate by `x` every 10 epochs\n",
        "### Classification Error on Test set for adaptive learning rates:\n",
        "For Starting Learning Rate = 0.01\n",
        "- x: 1.3, Time: 4.61761, Test error: 4.22222 %\n",
        "- x: 1.4, Time: 4.66165, Test error: 6.22222 %\n",
        "- x: 1.5, Time: 4.65058, Test error: 7.20833 %\n",
        "\n",
        "For Starting Learning Rate = 0.007\n",
        "- x: 1.3, Time: 4.52604, Test error: 7.76389 %\n",
        "- x: 1.4, Time: 4.63525, Test error: 8.54167 %\n",
        "- x: 1.5, Time: 4.79620, Test error: 10.08333 %\n",
        "\n",
        "For Starting Learning Rate = 0.005\n",
        "- x: 0.8, Time: 4.64237, Test error: 1.22222 %\n",
        "- x: 0.9, Time: 4.65208, Test error: 1.77778 %\n",
        "- x: 1.3, Time: 4.56385, Test error: 8.20833 %\n",
        "- x: 1.4, Time: 4.64512, Test error: 10.09722 %\n",
        "- x: 1.5, Time: 4.61243, Test error: 12.06945 %\n",
        "\n",
        "For Starting Learning Rate = 0.003\n",
        "- x: 0.8, Time: 4.60890, Test error: 3.44445 %\n",
        "- x: 0.9, Time: 4.66232, Test error: 5.00000 %"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JClV06wwsmE_"
      },
      "source": [
        "# Task 4: \n",
        "You are asked to study the effect of mini-batch size. You can set mini-batch size to be 1 (stochastic\n",
        "gradient descent), N (batch gradient descent) or any other size. The most important aspect is to be\n",
        "conclusive with your finding. The mini-batch size really depends on the problem size. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WCLBVwQlCzEb"
      },
      "source": [
        "### Changing mini-batch size"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mBSBLcSL9z22"
      },
      "source": [
        "# Hyper-parameters \n",
        "input_size = X_train.shape[1]   # 48\n",
        "output_size = len(Y_train.unique())   # 3 outcomes: 0, 1, 2\n",
        "hidden_size = 100  # no of nodes in hidden layers\n",
        "num_epochs = 100\n",
        "batch_size = 50\n",
        "learning_rate = 0.001\n",
        "\n",
        "# Make a fully connected neural network with 3 layers\n",
        "class FeedForward(nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size, hidden_size):\n",
        "        super(FeedForward, self).__init__()\n",
        "        self.inputs = input_size\n",
        "        self.outputs = output_size\n",
        "        self.hidden = hidden_size\n",
        "\n",
        "        # Define the layers\n",
        "        self.fc1 = nn.Linear(self.inputs, self.hidden, bias = False)\n",
        "        self.fc2 = nn.Linear(self.hidden, self.hidden, bias = False)\n",
        "        self.fc3 = nn.Linear(self.hidden, self.outputs, bias = False)\n",
        "        \n",
        "    def forward(self, x):\n",
        "        output = self.fc1(x)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc2(output)\n",
        "        output = F.relu(output)\n",
        "        output = self.fc3(output)\n",
        "        return output"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_EuF-oN09z23",
        "outputId": "67d78d81-96d7-400d-db5f-31326bd9bb52"
      },
      "source": [
        "model = FeedForward(input_size, output_size, hidden_size)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(params = model.parameters() , lr = learning_rate)\n",
        "\n",
        "start = time.time()\n",
        "\n",
        "# Train the model\n",
        "for epoch in range(num_epochs):\n",
        "\n",
        "    running_loss = 0\n",
        "    running_error = 0\n",
        "    num_batches = 0\n",
        "    \n",
        "    shuffled_indices = torch.randperm(X_train.size()[0])\n",
        " \n",
        "    for count in range(0, X_train.size()[0], batch_size):\n",
        "\n",
        "        # forward and backward pass\n",
        "        optimizer.zero_grad()\n",
        "        \n",
        "        indices=shuffled_indices[count:count+batch_size]\n",
        "        minibatch_data =  X_train[indices]\n",
        "        minibatch_label = Y_train[indices]\n",
        "\n",
        "        inputs = minibatch_data.view(minibatch_data.size(),48)\n",
        "        inputs.requires_grad_()\n",
        "        outputs = model(inputs) \n",
        "\n",
        "        loss = criterion(outputs, minibatch_label)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        \n",
        "        # compute some stats\n",
        "        running_loss += loss.detach().item()\n",
        "        error = get_error(outputs.detach() , minibatch_label)\n",
        "        running_error += error.item()\n",
        "        \n",
        "        num_batches += 1\n",
        "    \n",
        "    # once the epoch is finished we divide the \"running quantities\" by the number of batches\n",
        "    total_loss = running_loss/num_batches\n",
        "    total_error = running_error/num_batches\n",
        "    elapsed_time = time.time() - start\n",
        "    \n",
        "    # every 10 epoch we display the stats and compute the error rate on the test set  \n",
        "    if epoch % 10 == 0 : \n",
        "\n",
        "        print(\"For Num. of Epochs: {:.0f}, Time: {:.5f}, Loss: {:.5f}, Learning Rate: {:.5f}, Error: {:.5f} %\".format(epoch, elapsed_time, total_loss, learning_rate, total_error * 100))\n",
        "        eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For Num. of Epochs: 0, Time: 0.03053, Loss: 1.09714, Learning Rate: 0.00100, Error: 65.38690 %\n",
            "Test error: 64.66049 %\n",
            "For Num. of Epochs: 10, Time: 0.32883, Loss: 1.09269, Learning Rate: 0.00100, Error: 61.10119 %\n",
            "Test error: 59.61728 %\n",
            "For Num. of Epochs: 20, Time: 0.62057, Loss: 1.08697, Learning Rate: 0.00100, Error: 52.92857 %\n",
            "Test error: 51.88889 %\n",
            "For Num. of Epochs: 30, Time: 0.92437, Loss: 1.08270, Learning Rate: 0.00100, Error: 46.25000 %\n",
            "Test error: 45.17901 %\n",
            "For Num. of Epochs: 40, Time: 1.22828, Loss: 1.07742, Learning Rate: 0.00100, Error: 41.95833 %\n",
            "Test error: 40.17901 %\n",
            "For Num. of Epochs: 50, Time: 1.52014, Loss: 1.07216, Learning Rate: 0.00100, Error: 40.73214 %\n",
            "Test error: 39.29012 %\n",
            "For Num. of Epochs: 60, Time: 1.83593, Loss: 1.06532, Learning Rate: 0.00100, Error: 39.03571 %\n",
            "Test error: 38.80247 %\n",
            "For Num. of Epochs: 70, Time: 2.13391, Loss: 1.05901, Learning Rate: 0.00100, Error: 38.66667 %\n",
            "Test error: 37.60494 %\n",
            "For Num. of Epochs: 80, Time: 2.42861, Loss: 1.05164, Learning Rate: 0.00100, Error: 38.02976 %\n",
            "Test error: 37.49383 %\n",
            "For Num. of Epochs: 90, Time: 2.72462, Loss: 1.04298, Learning Rate: 0.00100, Error: 38.97619 %\n",
            "Test error: 38.42593 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IMGVQJRcAriz",
        "outputId": "df93e169-6322-4f74-ecb1-4596839d2a46"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 12.06189,     mini batch size: 10,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 7.71536 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VKKJU82x9z23",
        "outputId": "8edec6bc-d688-4019-ee04-5a646c23be4f"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 6.64465,     mini batch size: 20,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 13.14815 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d9RoaHyK-MXP",
        "outputId": "e0589f69-58be-49ac-ffc4-1bee6c93c0e0"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 4.61398,     mini batch size: 30,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 29.48611 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rAxHS8p5_ALg",
        "outputId": "0dc8fc8d-2fc6-4f81-f1e4-4d68388989d0"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 3.64458,     mini batch size: 40,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 38.04348 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A7_PAV6k-mPt",
        "outputId": "ba3e2061-2d31-428e-c41b-c713e35a0da5"
      },
      "source": [
        "print(\"For num of epochs: {:.0f}, \\\n",
        "    Time: {:.5f}, \\\n",
        "    mini batch size: {:.0f}, \\\n",
        "    learning rate: {:.4f}, \\\n",
        "    num of hidden nodes: {:.0f}\".format(num_epochs, elapsed_time, batch_size, learning_rate, hidden_size))\n",
        "eval_on_test_set()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "For num of epochs: 100,     Time: 3.00074,     mini batch size: 50,     learning rate: 0.0010,     num of hidden nodes: 100\n",
            "Test error: 38.42593 %\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V1EXYY8TCwXi"
      },
      "source": [
        "### Results: \n",
        "### Constant hyper parameters used: \n",
        "\n",
        "  1.   Number of Epochs = 100\n",
        "  2.   Learning rates = 0.001 (Fixed)\n",
        "  3.   Number of hidden nodes = 100\n",
        "\n",
        "\n",
        "### Classification Error on Test set for mini batch size: \n",
        "- mini batch size: 10, Time: 12.06189, Test error: 7.71536 %\n",
        "- mini batch size: 20, Time: 6.64465, Test error: 13.14815 %\n",
        "- mini batch size: 30, Time: 4.61398, Test error: 29.48611 %\n",
        "- mini batch size: 40, Time: 3.64458, Test error: 38.04348 %\n",
        "- mini batch size: 50, Time: 3.00074, Test error: 38.42593 %\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2RmIpmR5smaK"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}